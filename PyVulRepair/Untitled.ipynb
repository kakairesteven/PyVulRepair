{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2e23f9-41e8-4c10-a586-8de8e888d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10d3493-94bb-4571-8dab-6cbcaca89dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\"train\":\"train.parquet\",\n",
    "#               \"test\":\"test.parquet\"\n",
    "#              }\n",
    "# df = datasets.load_dataset('AfricaKing/TSSB-3M',\n",
    "#                      # data_files=data_files\n",
    "#                            split='train'\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6673b6ab-f415-4ee0-b778-ba79c1480f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source feature\n",
    "# len(df['before'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d427b1-713d-465a-8905-73705201e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a split for the dataset\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ef87b1-7992-466e-bcdf-19b2cbea308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--model_type MODEL_TYPE]\n",
      "                             [--encoder_block_size ENCODER_BLOCK_SIZE]\n",
      "                             [--decoder_block_size DECODER_BLOCK_SIZE]\n",
      "                             [--num_beams NUM_BEAMS] [--model_name MODEL_NAME]\n",
      "                             [--checkpoint_model_name CHECKPOINT_MODEL_NAME]\n",
      "                             [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--config_name CONFIG_NAME]\n",
      "                             [--tokenizer_name TOKENIZER_NAME] [--do_train]\n",
      "                             [--do_test] [--evaluate_during_training]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--warmup_steps WARMUP_STEPS] [--seed SEED]\n",
      "                             [--epochs EPOCHS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/africa/.local/share/jupyter/runtime/kernel-2049c8a8-b1e8-4e63-9bfe-6600de198b18.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africa/Desktop/PyVulRepair/PyVulRepair/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from transformers import (AdamW, get_linear_schedule_with_warmup, \n",
    "                          T5ForConditionalGeneration, RobertaTokenizer)\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cpu_cont = 16\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 label,\n",
    "                 decoder_input_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.label=label\n",
    "        self.decoder_input_ids = decoder_input_ids\n",
    "        \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, train_data=None,\n",
    "                 val_data=None,\n",
    "                 file_type=\"train\"):\n",
    "        if file_type == \"train\":\n",
    "\n",
    "            # Train dataset: Source\n",
    "            sources = train_data[\"before\"].tolist()\n",
    "\n",
    "            # Train dataset: Target\n",
    "            labels = train_data[\"after\"].tolist()\n",
    "            \n",
    "        elif file_type == \"eval\":\n",
    "            sources = val_data[\"before\"].tolist()\n",
    "            labels = val_data[\"after\"].tolist()\n",
    "            \n",
    "        elif file_type == \"test\":\n",
    "            # Read dataset as a test split\n",
    "            data = datasets.load_dataset('AfricaKing/TSSB-3M', split=\"test\")\n",
    "            sources = data[\"before\"]\n",
    "            labels = data[\"after\"]\n",
    "        self.examples = []\n",
    "        \n",
    "        for i in tqdm(range(len(sources))):\n",
    "            self.examples.append(convert_examples_to_features(sources[i], labels[i], tokenizer, args))\n",
    "        if file_type == \"train\":\n",
    "            for example in self.examples[:3]:\n",
    "                    logger.info(\"*** Example ***\")\n",
    "                    logger.info(\"label: {}\".format(example.label))\n",
    "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
    "                    logger.info(\"decoder_input_ids: {}\".format(' '.join(map(str, example.decoder_input_ids))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return self.examples[i].input_ids, self.examples[i].input_ids.ne(0), self.examples[i].label, self.examples[i].decoder_input_ids\n",
    "\n",
    "\n",
    "def convert_examples_to_features(source, label, tokenizer, args):\n",
    "    # encode - subword tokenize\n",
    "    source_ids = tokenizer.encode(source, truncation=True, max_length=args.encoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    decoder_input_ids = tokenizer.encode(label, truncation=True, max_length=args.decoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    label = tokenizer.encode(label, truncation=True, max_length=args.decoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    return InputFeatures(source_ids, label, decoder_input_ids)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer, eval_dataset):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # build dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n",
    "    \n",
    "    args.max_steps = args.epochs * len(train_dataloader)\n",
    "\n",
    "    # evaluate model per epoch\n",
    "    args.save_steps = len(train_dataloader) * 1\n",
    "   \n",
    "    args.warmup_steps = args.max_steps // 5\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=args.max_steps)\n",
    "    \n",
    "    # multi-gpu training\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size//max(args.n_gpu, 1))\n",
    "    logger.info(\"  Total train batch size = %d\",args.train_batch_size*args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", args.max_steps)\n",
    "    \n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0\n",
    "    best_loss = 100\n",
    "\n",
    "    writer_path = \"tb/codet5_training_loss\"\n",
    "    writer = SummaryWriter(writer_path)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx in range(args.epochs): \n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        tr_num = 0\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(bar):\n",
    "            (input_ids, attention_mask, labels, decoder_input_ids) = [x.squeeze(1).to(args.device) for x in batch]\n",
    "            model.train()\n",
    "            # the forward function automatically creates the correct decoder_input_ids\n",
    "            loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            tr_num += 1\n",
    "            train_loss += loss.item()\n",
    "            if avg_loss == 0:\n",
    "                avg_loss = tr_loss\n",
    "            avg_loss = round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "            \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  \n",
    "                global_step += 1\n",
    "                output_flag = True\n",
    "                avg_loss = round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "                if global_step % args.save_steps == 0:\n",
    "                    # placeholder of evaluation\n",
    "                    eval_loss = evaluate(args, model, tokenizer, eval_dataset, eval_when_training=True)    \n",
    "                    # Save model checkpoint\n",
    "                    if eval_loss < best_loss:\n",
    "                        best_loss = eval_loss\n",
    "                        logger.info(\"  \"+\"*\"*20)  \n",
    "                        logger.info(\"  Best Loss:%s\",round(best_loss,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)                          \n",
    "                        checkpoint_prefix = 'checkpoint-best-loss'\n",
    "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))                        \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)                        \n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format(args.model_name)) \n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    tokens = tokens.replace(\"<pad>\", \"\")\n",
    "    tokens = tokens.replace(\"<s>\", \"\")\n",
    "    tokens = tokens.replace(\"</s>\", \"\")\n",
    "    tokens = tokens.strip(\"\\n\")\n",
    "    tokens = tokens.strip()\n",
    "    return tokens\n",
    "\n",
    "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n",
    "    #build dataloader\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, num_workers=0)\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and eval_when_training is False:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, num = 0, 0\n",
    "    bar = tqdm(eval_dataloader, total=len(eval_dataloader))\n",
    "    for batch in bar:\n",
    "        (input_ids, attention_mask, labels, decoder_input_ids) = [x.squeeze(1).to(args.device) for x in batch]\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        if args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        eval_loss += loss.item()\n",
    "        num += 1\n",
    "    eval_loss = round(eval_loss/num,5)\n",
    "    model.train()\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    logger.info(f\"Evaluation Loss: {str(eval_loss)}\")\n",
    "    return eval_loss\n",
    "\n",
    "def test(args, model, tokenizer, test_dataset, best_threshold=0.5):\n",
    "    # build dataloader\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    # Test!\n",
    "    logger.info(\"***** Running Test *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    accuracy = []\n",
    "    raw_predictions = []\n",
    "    correct_prediction = \"\"\n",
    "    bar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "    for batch in bar:\n",
    "        correct_pred = False\n",
    "        (input_ids, attention_mask, labels, decoder_input_ids)=[x.squeeze(1).to(args.device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            beam_outputs = model.generate(input_ids=input_ids,\n",
    "                                          attention_mask=attention_mask,\n",
    "                                          do_sample=False, # disable sampling to test if batching affects output\n",
    "                                          num_beams=args.num_beams,\n",
    "                                          num_return_sequences=args.num_beams,\n",
    "                                          max_length=args.decoder_block_size)\n",
    "        beam_outputs = beam_outputs.detach().cpu().tolist()\n",
    "        decoder_input_ids = decoder_input_ids.detach().cpu().tolist()\n",
    "        for single_output in beam_outputs:\n",
    "            # pred\n",
    "            prediction = tokenizer.decode(single_output, skip_special_tokens=False)\n",
    "            prediction = clean_tokens(prediction)\n",
    "            # truth\n",
    "            ground_truth = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=False)\n",
    "            ground_truth = clean_tokens(ground_truth)\n",
    "            if prediction == ground_truth:\n",
    "                correct_prediction = prediction\n",
    "                correct_pred = True\n",
    "                break\n",
    "        if correct_pred:\n",
    "            raw_predictions.append(correct_prediction)\n",
    "            accuracy.append(1)\n",
    "        else:\n",
    "            # if not correct, use the first output in the beam as the raw prediction\n",
    "            raw_pred = tokenizer.decode(beam_outputs[0], skip_special_tokens=False)\n",
    "            raw_pred = clean_tokens(raw_pred)\n",
    "            raw_predictions.append(raw_pred)\n",
    "            accuracy.append(0)\n",
    "        nb_eval_steps += 1\n",
    "    # calculate accuracy\n",
    "    test_result = round(sum(accuracy) / len(accuracy), 4)\n",
    "    logger.info(\"***** Test results *****\")\n",
    "    logger.info(f\"Test Accuracy: {str(test_result)}\")\n",
    "\n",
    "    # write prediction to file\n",
    "    df = pd.DataFrame({\"raw_predictions\": [], \"correctly_predicted\": []})\n",
    "    df[\"raw_predictions\"] = raw_predictions\n",
    "    df[\"correctly_predicted\"] = accuracy\n",
    "    df.to_csv(\"./raw_predictions/PyVulRepair_raw_preds.csv\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Params\n",
    "    parser.add_argument(\"--output_dir\", default=None, type=str, required=False,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "    parser.add_argument(\"--model_type\", default=\"t5\", type=str,\n",
    "                        help=\"The model architecture to be fine-tuned.\")\n",
    "    parser.add_argument(\"--encoder_block_size\", default=-1, type=int,\n",
    "                        help=\"Optional input sequence length after tokenization.\"\n",
    "                             \"The training dataset will be truncated in block of this size for training.\"\n",
    "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "    parser.add_argument(\"--decoder_block_size\", default=-1, type=int,\n",
    "                        help=\"Optional input sequence length after tokenization.\"\n",
    "                             \"The training dataset will be truncated in block of this size for training.\"\n",
    "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "    parser.add_argument(\"--num_beams\", default=50, type=int,\n",
    "                        help=\"Beam size to use when decoding.\")                          \n",
    "    parser.add_argument(\"--model_name\", default=\"model.bin\", type=str,\n",
    "                        help=\"Saved model name.\")\n",
    "    parser.add_argument(\"--checkpoint_model_name\", default=\"non_domain_model.bin\", type=str,\n",
    "                            help=\"Checkpoint model name.\")\n",
    "    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n",
    "                        help=\"The model checkpoint for weights initialization.\")\n",
    "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "\n",
    "    parser.add_argument(\"--do_train\", action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_test\", action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
    "                        help=\"Run evaluation during training at each logging step.\")\n",
    "\n",
    "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                        help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                        help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                        help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--epochs', type=int, default=1,\n",
    "                        help=\"training epochs\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Setup CUDA, GPU, CPU\n",
    "    # torch.cuda.is_available = lambda : False\n",
    "    device = torch.device(\n",
    "        \"cuda:0\" if torch.cuda.is_available() else \n",
    "        \"cpu\")\n",
    "    args.n_gpu = 0\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',datefmt='%m/%d/%Y %H:%M:%S',level=logging.INFO)\n",
    "    logger.warning(\"device: %s, n_gpu: %s\",device, args.n_gpu,)\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    tokenizer.add_tokens([\"<S2SV_StartBug>\", \"<S2SV_EndBug>\", \"<S2SV_blank>\", \"<S2SV_ModStart>\", \"<S2SV_ModEnd>\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(args.model_name_or_path) \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        # Training the model on 3M TSSB bugs\n",
    "        train_data_whole = datasets.load_dataset(\"AfricaKing/TSSB-3M\", split=\"train\")\n",
    "        df = pd.DataFrame({\"before\": train_data_whole[\"before\"], \"after\": train_data_whole[\"after\"]})\n",
    "\n",
    "        # Splitting the dataset\n",
    "        train_data, val_data = train_test_split(df, test_size=.30, random_state=42)\n",
    "        train_dataset = TextDataset(tokenizer, args, train_data,\n",
    "                                    val_data,\n",
    "                                    file_type='train')\n",
    "        eval_dataset = TextDataset(tokenizer, args, df,\n",
    "                                   val_data,\n",
    "                                   file_type='eval')\n",
    "        train(args, train_dataset, model, tokenizer, eval_dataset)\n",
    "    \n",
    "    # Evaluation\n",
    "    results = {}  \n",
    "    if args.do_test:\n",
    "        if args.model_name_or_path != \"AfricaKing/PyVulRepair\":\n",
    "            checkpoint_prefix = f'checkpoint-best-loss/{args.model_name}'\n",
    "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n",
    "            model.load_state_dict(torch.load(output_dir, map_location=args.device))\n",
    "        model.to(args.device)\n",
    "        test_dataset = TextDataset(tokenizer, args, file_type='test')\n",
    "        test(args, model, tokenizer, test_dataset, best_threshold=0.5)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c141a31-c890-4aa4-8d45-f78d25767fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e549f30d304d82addcd11f79c5420b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if len ( op . metadata [ 'device_id' ] ) == 1 ...</td>\n",
       "      <td>if 'device_id' in op . metadata and isinstance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raise ValueError ( 'Invalid slice in item {}' ...</td>\n",
       "      <td>raise ValueError ( 'Invalid slice (negative st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def copy_op_with_new_args ( self , args ) : re...</td>\n",
       "      <td>def copy_with_new_args ( self , args ) : retur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>op . metadata [ 'device_id' ] = '1'</td>\n",
       "      <td>op . metadata [ 'device_id' ] = op . metadata ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if self . transformer_name == 'hetr' : pytest ...</td>\n",
       "      <td>if self . transformer_name in [ 'hetr' , 'gpu'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441227</th>\n",
       "      <td>samples = ids [ : , : top_beams , 1 ]</td>\n",
       "      <td>samples = ids [ : , : top_beams , 1 : ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441228</th>\n",
       "      <td>problem = hparams . problem_instances [ 0 ] or...</td>\n",
       "      <td>problem = get_problem_from_hparams ( hparams )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441229</th>\n",
       "      <td>problem_hparams = hparams . problems [ problem...</td>\n",
       "      <td>problem_hparams = hparams . problem_hparams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441230</th>\n",
       "      <td>opt_summaries = [ \"loss\" , \"global_gradient_no...</td>\n",
       "      <td>opt_summaries = [ \"loss\" ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441231</th>\n",
       "      <td>rhp . set_float ( \"weight_decay\" , 0.0 , 2.0 )</td>\n",
       "      <td>rhp . set_float ( \"weight_decay\" , 0.0 , 1e-4 )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3441232 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    before  \\\n",
       "0        if len ( op . metadata [ 'device_id' ] ) == 1 ...   \n",
       "1        raise ValueError ( 'Invalid slice in item {}' ...   \n",
       "2        def copy_op_with_new_args ( self , args ) : re...   \n",
       "3                      op . metadata [ 'device_id' ] = '1'   \n",
       "4        if self . transformer_name == 'hetr' : pytest ...   \n",
       "...                                                    ...   \n",
       "3441227              samples = ids [ : , : top_beams , 1 ]   \n",
       "3441228  problem = hparams . problem_instances [ 0 ] or...   \n",
       "3441229  problem_hparams = hparams . problems [ problem...   \n",
       "3441230  opt_summaries = [ \"loss\" , \"global_gradient_no...   \n",
       "3441231     rhp . set_float ( \"weight_decay\" , 0.0 , 2.0 )   \n",
       "\n",
       "                                                     after  \n",
       "0        if 'device_id' in op . metadata and isinstance...  \n",
       "1        raise ValueError ( 'Invalid slice (negative st...  \n",
       "2        def copy_with_new_args ( self , args ) : retur...  \n",
       "3        op . metadata [ 'device_id' ] = op . metadata ...  \n",
       "4        if self . transformer_name in [ 'hetr' , 'gpu'...  \n",
       "...                                                    ...  \n",
       "3441227            samples = ids [ : , : top_beams , 1 : ]  \n",
       "3441228     problem = get_problem_from_hparams ( hparams )  \n",
       "3441229        problem_hparams = hparams . problem_hparams  \n",
       "3441230                         opt_summaries = [ \"loss\" ]  \n",
       "3441231    rhp . set_float ( \"weight_decay\" , 0.0 , 1e-4 )  \n",
       "\n",
       "[3441232 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_whole = datasets.load_dataset(\"AfricaKing/TSSB-3M\", split=\"train\")\n",
    "df = pd.DataFrame({\"before\": train_data_whole[\"before\"], \"after\": train_data_whole[\"after\"]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b57f559-851a-462b-a7a6-e6ff98fc578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='Salesforce/codet5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': [AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=True, single_word=False, normalized=True), AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=True, single_word=False, normalized=True)]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(df, test_size=.30, random_state=42)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba4691e-3bb4-482a-b798-d1d4886cdd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer.add_tokens([\"<S2SV_StartBug>\", \"<S2SV_EndBug>\", \"<S2SV_blank>\", \"<S2SV_ModStart>\", \"<S2SV_ModEnd>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84487de7-75a6-46c7-b2ce-adc59dea1249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d4e111-740b-49ba-89fa-76c0e9641fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='model.bin'     \n",
    "output_dir='./saved_models'     \n",
    "tokenizer_name='Salesforce/codet5-base'\n",
    "model_name_or_path='Salesforce/codet5-base'\n",
    "do_train = True\n",
    "epochs = 75\n",
    "encoder_block_size=512\n",
    "decoder_block_size=256\n",
    "train_batch_size=4\n",
    "eval_batch_size=4\n",
    "learning_rate=2e-5\n",
    "max_grad_norm=1.0 \n",
    "evaluate_during_training = True\n",
    "seed=123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a9771a-6d64-46bf-8a07-a37ef0b8d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer,\n",
    "                 train_data=None,\n",
    "                 val_data=None,\n",
    "                 file_type=\"train\"):\n",
    "        if file_type == \"train\":\n",
    "\n",
    "            # Train dataset: Source\n",
    "            sources = train_data[\"before\"].tolist()\n",
    "\n",
    "            # Train dataset: Target\n",
    "            labels = train_data[\"after\"].tolist()\n",
    "            \n",
    "        elif file_type == \"eval\":\n",
    "            sources = val_data[\"before\"].tolist()\n",
    "            labels = val_data[\"after\"].tolist()\n",
    "            \n",
    "        elif file_type == \"test\":\n",
    "            # Read dataset as a test split\n",
    "            data = datasets.load_dataset('AfricaKing/TSSB-3M', split=\"test\")\n",
    "            sources = data[\"before\"]\n",
    "            labels = data[\"after\"]\n",
    "        self.examples = []\n",
    "        \n",
    "        for i in tqdm(range(len(sources))):\n",
    "            self.examples.append(convert_examples_to_features(sources[i], labels[i], tokenizer))\n",
    "        if file_type == \"train\":\n",
    "            for example in self.examples[:3]:\n",
    "                    logger.info(\"*** Example ***\")\n",
    "                    logger.info(\"label: {}\".format(example.label))\n",
    "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
    "                    logger.info(\"decoder_input_ids: {}\".format(' '.join(map(str, example.decoder_input_ids))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return self.examples[i].input_ids, self.examples[i].input_ids.ne(0), self.examples[i].label, self.examples[i].decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce713169-f506-4f33-9999-4c82b9dfc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(source, label, tokenizer):\n",
    "    # encode - subword tokenize\n",
    "    source_ids = tokenizer.encode(source, truncation=True, max_length=encoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    decoder_input_ids = tokenizer.encode(label, truncation=True, max_length=decoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    label = tokenizer.encode(label, truncation=True, max_length=decoder_block_size, padding='max_length', return_tensors='pt')\n",
    "    return InputFeatures(source_ids, label, decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3599b2-ec1e-4058-a370-d98596147700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb78f0-7f5c-4430-aef2-bd7c3453ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–‹                                                                                                                                                | 12310/2408862 [00:08<25:27, 1569.04it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer,train_data,\n",
    "                                    val_data,\n",
    "                                    file_type='train')\n",
    "eval_dataset = TextDataset(tokenizer, df,\n",
    "                                   val_data,\n",
    "                                   file_type='eval')\n",
    "\n",
    "# train(args, train_dataset, model, tokenizer, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e885bd-63d6-4d05-8b1c-49696ca0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14195c20-626f-4b14-b0e9-b6e0a41152cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
